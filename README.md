# 24â€¯Pointâ€¯GameÂ ğŸ¯

> **Author:** Sihengâ€¯Li  
> **Date:** 2025â€‘04â€‘17  
> A simple *O(1)* 24â€‘point solver, trained with **SFT**, **SFTâ€¯+â€¯RL**, and **pureâ€¯RL**, surpassing **70â€¯%** accuracy.

---

## Tableâ€¯ofâ€¯Contents

1. [DataÂ Preparation](#1-data-preparation-ğŸ”)  
2. [Training](#2-training-ğŸ‹ï¸)  
3. [Evaluation](#3-evaluation-ğŸ“)  
4. [ResultsÂ &Â Analysis](#4-results--analysis-ğŸ“Š)  
5. [ContactÂ &Â Acknowledgements](#5-contact--acknowledgements)

---

## 1Â Â·Â DataÂ PreparationÂ ğŸ” <a id="1-data-preparation-ğŸ”"></a>

We generate **100â€¯000** valid fourâ€‘integer puzzles (values âˆˆÂ [1â€¯â€¦â€¯99]) that can yield exactly **24**.

| Step | Description |
|------|-------------|
| **Enumeration** | List every 4â€‘integer combination in `[1,â€¯99]`. |
| **Filtering** | Keep only tuples with **â‰¥â€¯1** valid 24â€‘point expression. |
| **Scripts** | `build_prompt.py`, `build_response.py` |

### 1.1Â Finding Valid Combinations

```python
from fractions import Fraction
from itertools import permutations
from your_module import ops_expr

def find_expression_for_24(nums):
    """Return a string expression that equals 24, or None."""
    target = Fraction(24, 1)

    for perm in permutations(nums):
        A = (Fraction(perm[0]), str(perm[0]))
        B = (Fraction(perm[1]), str(perm[1]))
        C = (Fraction(perm[2]), str(perm[2]))
        D = (Fraction(perm[3]), str(perm[3]))

        # PatternÂ 1: ((aÂ opÂ b)Â opÂ c)Â opÂ d
        for v1, e1 in ops_expr(A[0], A[1], B[0], B[1]):
            for v2, e2 in ops_expr(v1, e1, C[0], C[1]):
                for v3, e3 in ops_expr(v2, e2, D[0], D[1]):
                    if v3 == target:
                        return e3
        #Â â€¦Â PatternsÂ 2â€“5 omitted for brevity
    return None
```

### 1.2Â Humanâ€‘LikeÂ ThoughtÂ ProcessesÂ ğŸ§ 

```python
from fractions import Fraction
import random
from your_module import (
    fmt,
    generate_pairwise_ops,
    check_whether_can_make_24,
    exploration_two,
)

def solve_24_response(
    numbers,
    exploration_three_prob=0.30,
    exploration_two_prob=0.30,
    error_prob=0.10,
):
    """Return <think>â€¦</think><answer>â€¦</answer> with logs & optional errors."""
    items = [(Fraction(n), str(n)) for n in numbers]
    thought = [f"Starting with values: {numbers}"]
    answer_expr = None

    #Â â€¦Â recursive solver logic â€¦

    thought = ["<think>", *thought, "</think>", f"<answer>{answer_expr}</answer>"]
    return "\n".join(thought)
```

<details>
<summary>Example (with â€œWaitâ€Â logs)</summary>

```text
<think>
Starting with values: [35, 12, 53, 16]
Try 35+12 = 47 â†’ [53, 16, 47] â€“ cannot obtain 24
Try 35-12 = 23 â†’ [53, 16, 23] â€“ cannot obtain 24
Try 35*12 = 420 â†’ [53, 16, 420]
Wait.
Consider 53-16 = 37, remaining 420 â†’ cannot reach 24
â€¦
Compute (53-35)/(12/16) = 18 Ã· 3/4 = 24 âœ“
</think>
<answer>(53-35)/(12/16)</answer>
```

</details>

---

## 2Â Â·Â TrainingÂ ğŸ‹ï¸ <a id="2-training-ğŸ‹ï¸"></a>

### 2.1Â SupervisedÂ Fineâ€‘TuningÂ (SFT)Â ğŸ“š

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model_args.model_name_or_path,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    peft_config=get_peft_config(model_args),
    processing_class=tokenizer,
)
```

```bash
sbatch train_sft.slurm
```

### 2.2Â ReinforcementÂ LearningÂ (GRPO)Â ğŸ¤–

```python
from trl import GRPOTrainer

trainer = GRPOTrainer(
    model=model_args.model_name_or_path,
    reward_funcs=reward_funcs,  # discourages â€œhacksâ€
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    peft_config=get_peft_config(model_args),
    processing_class=tokenizer,
)
```

```bash
sbatch train_grpo.slurm
```

---

## 3Â Â·Â EvaluationÂ ğŸ“ <a id="3-evaluation-ğŸ“"></a>

We sample **16** completions per prompt and compute **Pass@â€¯K** for  
*KÂ âˆˆÂ {1,â€¯2,â€¯4,â€¯8,â€¯16}*.

```python
def compute_metrics(dataset, num_samples=16):
    #Â calculate Pass@K and attach correctness flags
    return metrics, dataset_with_flags
```

```bash
bash evaluate.sh
```

---

## 4Â Â·Â ResultsÂ &Â AnalysisÂ ğŸ“Š <a id="4-results--analysis-ğŸ“Š"></a>

### 4.1Â MainÂ Results

| Model | Pass@1 | Pass@2 | Pass@4 | Pass@8 | Pass@16 |
|-------|-------:|-------:|-------:|-------:|--------:|
| Qwen2.5â€‘1.5Bâ€‘Instruct | 0.00Â |Â 0.00 |Â 0.00 | 0.00Â | 0.03Â |
| DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘1.5B |Â 0.04 | 0.08Â | 0.12Â | 0.21Â |Â 0.33 |
| Qwen2.5â€‘1.5B + SFT  |Â 0.40 | 0.66Â | 0.80Â |Â 0.88 |Â 0.91 |
| Qwen2.5â€‘1.5B + SFT + RL  |Â |Â |Â |Â |Â |
| Qwen2.5â€‘1.5B + RLÂ only |Â |Â |Â |Â |Â |

### 4.2Â Impactâ€¯ofâ€¯Testâ€‘Timeâ€¯ComputeÂ â±ï¸

Discuss how increasing *K* improves accuracy at the cost of latency and compute. Include any Pareto observations or scaling laws you observed.

---

## 5Â Â·Â ContactÂ &Â Acknowledgements <a id="5-contact--acknowledgements"></a>

| | |
|---|---|
| **Name** | Sihengâ€¯Li |
| **Email** | sihengli24@gmail.com |
| **GitHub** | <https://github.com/your-username/24-point-game> |

Feel free to open an issue or reach outâ€”happy solvingâ€¯!Â ğŸ‰
